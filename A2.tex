\documentclass[11pt, answers]{exam}
\renewcommand{\baselinestretch}{1.05}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{graphics}

\usepackage{afterpage}
\usepackage{caption}

\usepackage{fancybox}

\usepackage{clrscode3e}
\usepackage{amsmath}
\usepackage{amssymb}

\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm
\theoremstyle{plain}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem*{surfacecor}{Corollary 1}
\newtheorem{conjecture}{Conjecture}  
\newtheorem{definition}{Definition}
\newcommand{\R}{\mathbb{R}}

\title{CSC336: Assignment 2}
\author{Junjie Cheng, 1002770539}
\date{November 1 2017}

\begin{document}

\maketitle

\begin{questions}
\question %Q1
Let $x$ be an arbitrary number in $\R^n$ for arbitrary $n \ge 1$, by definition, $||x||_2 = \sqrt{\Sigma_{i=1}^n x_i^2} $. 

By customary we define $\Sigma_{k=n}^m$ with $n>m$ as the empty sum (i.e. $0$). Then we have

$||x||_1 = \Sigma_{i=1}^n |x_i| = \sqrt{(\Sigma_{i=1}^n |x_i|)^2} = \sqrt{\Sigma_{i=1}^n x_i^2 + 2\Sigma_{i=1}^n \Sigma_{j=i+1}^n |x_i||x_j|}$.

Since $|x_i| \ge 0$ for all $1 \le i \le n$, $|x_i||x_j| \ge 0$ for all $1 \le i \le j \le n$. So, $2\Sigma_{i=1}^n \Sigma_{j=i+1}^n |x_i||x_j| \ge 0$ for all $n \ge 1$.

Then we have $||x||_2 = \sqrt{\Sigma_{i=1}^n x_i^2} \le \sqrt{\Sigma_{i=1}^n x_i^2 + 2\Sigma_{i=1}^n \Sigma_{j=i+1}^n |x_i||x_j|} = ||x||_1$.

\question %Q2
It is possible. Here is an example.

$x = (48, 55)^T, ||x||_1 = |48|+|55|= 103, ||x||_2 = \sqrt{48^2+55^2} = 73$;

$y = (13, 84)^T, ||y||_1 = |13|+|84|= 97,  ||y||_2 = \sqrt{13^2+84^2} = 85$;

$||x||_1 = 103 > 97 = ||y||_1$ while $||x||_2 = 73 < 85 = ||y||_2$.

\question %Q3
Since $Ax = b$ and $A\hat{x} = \hat{b}$, we have
$A(x-\hat{x}) = b - \hat{b}$. 

So $||b-\hat{b}|| = ||A(x-\hat{x})|| \le ||A||\cdot||x-\hat{x}||$.

On the other hand, $A^{-1}b = x$, so $||x|| = ||A^{-1}b|| \le ||A^{-1}||\cdot||b||$.

Since all norms are equal or greater than zero, we have $||b-\hat{b}|| \cdot ||x|| \le ||A||\cdot||A^{-1}|| \cdot ||x-\hat{x}|| \cdot ||b||$, so $\frac{1}{||A||\cdot||A^{-1}||}\frac{||b-\hat{b}||}{||b||} \le \frac{||x-\hat{x}||}{||x||}$, i.e. $ \frac{1}{cond(A)}\frac{||b-\hat{b}||}{||b||} \le \frac{||x-\hat{x}||}{||x||}$.

\question %Q4
\begin{parts}
\part
See attachments for code and result.
\part
By $Af=b$, we have $f = A^{-1}b$, so $||f|| \le ||A^{-1}||\cdot||b||$.

Since $r = b - A\hat{f}$, we have $||r|| = ||b-A\hat{f}|| = ||Af - A\hat{f}|| = ||A(f-\hat{f})|| \le ||A|| \cdot ||f-\hat{f}||$.

Since all norms are non-negative, we have $||r||\cdot||f|| \le ||A||\cdot||A^{-1}||\cdot||f-\hat{f}||\cdot||b||$. So,

$\frac{1}{cond(A)}\cdot\frac{||r||}{||b||} \le \frac{||\hat{f}-f||}{||f||}$

On the other hand, since $r = b-A\hat{f} = Af - A\hat{f}$, we have $f-\hat{f} = A^{-1}r$. Then,

$||f-\hat{f}|| \le ||A^{-1}||\cdot||r||$. We also have $||b|| \le ||A||\cdot||f||$, so 

$||f-\hat{f}||\cdot||b|| \le ||A^{-1}||\cdot||A||\cdot||r||\cdot||f||$, and we have

$\frac{||\hat{f} - f||}{||f||} \le cond(A)\cdot\frac{||r||}{||b||}$.

In conclusion, we have $ \frac{1}{cond(A)}\cdot\frac{||r||}{||b||} \le \frac{||\hat{f} - f||}{||f||} \le cond(A)\cdot\frac{||r||}{||b||} $.

The calculation results are:

Lower bound of relative error is 1.925969e-17.

Upper bound of relative error is 2.088024e-15.

\end{parts}
\question %Q5
As we can see from the part 1 result, the largest $n$ we can have before the error is 100 percent is $12$. Note that since $||x||=1$ , the absolute error and the relative error of $\hat{x}$ are equal.

Examining the result of $cond(H)$ and $log(cond(H))$ in part 2, we find that $log(cond(H))$ and $n$ are collinear. 

By Linear Regression we can write $log(cond(H)) = 3.476950n-3.654113$, i.e. 

$cond(H) = e^{3.476950n-3.654113} = \frac{(e^{3.476950})^n}{e^{3.654113}}$. The condition number of $H$ is exponentially growing with respect to $n$.

We calculate the number of correct digits in the components of the computed solution as $\lfloor\log{||x||}\rfloor - \lfloor\log{(\kw{absolute error})}\rfloor = \lfloor\log{||x||}\rfloor - \lfloor\log{||x-\hat{x}||}\rfloor$. (i.e. the place of the most significant digit of $||x||$ $-$ the place of the most significant digit of absolute error).

We find that, the larger the conditional number is, the smaller the number of correct digits will be.


\end{questions}
\end{document}
